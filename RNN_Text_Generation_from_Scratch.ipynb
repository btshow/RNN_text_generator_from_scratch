{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Text_Generation_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btshow/RNN_text_generator_from_scratch/blob/main/RNN_Text_Generation_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_8wrLL_OB0i"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "data_path = '/your/data/path/to/dinos.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4ClV9dN1TX"
      },
      "source": [
        "def initialize_parameters(n_a, n_x, n_y):\n",
        "    Wax = np.random.randn(n_a, n_x)*0.01  # input to hidden weight matrix,  default: n_a x  n_x = n_a x vocab_size = 50 x 27\n",
        "    #print(f'The dimensions of initialized weight matrix Wax are {Wax.shape}')\n",
        "    Waa = np.random.randn(n_a, n_a)*0.01  # hidden to hidden weight matrix, default: n_a x  n_x = 50 x 50\n",
        "    #print(f'The dimensions of initialized weight matrix Waa are {Waa.shape}')\n",
        "    Wya = np.random.randn(n_y, n_a)*0.01  # hidden to output weight matrix, default: n_y x  n_a = vocab_size x n_a = 50 x 27\n",
        "    #print(f'The dimensions of initialized weight matrix Wya are {Wya.shape}')\n",
        "    ba = np.zeros((n_a, 1))  # hidden biases vector, default: ba =  n_a x 1 = 50 x 1\n",
        "    #print(f'The dimensions of initialized bias vector ba are {ba.shape}')\n",
        "    by = np.zeros((n_y, 1))  # output biases vector,  default: ba =  n_a x 1 = 27 x 1\n",
        "    #print(f'The dimensions of initialized bias vector by are {by.shape}')\n",
        "\n",
        "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by} # collect parameters into a dictionary to have them all in one clear place\n",
        "    return parameters\n",
        "    \n",
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
        "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "    return a_next, yt_pred, cache\n",
        "\n",
        "\n",
        "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
        "    ''' \n",
        "    Performs the forward pass through the full RNN and also computes the cross-entropy loss.\n",
        "    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\n",
        "    '''\n",
        "    x, a, y_hat = {}, {}, {}\n",
        "    a[-1] = np.copy(a0)\n",
        "    loss = 0\n",
        "\n",
        "    # Encode the elements of the input sequence as dummy vectors: \n",
        "    # The elements of the input vector X are integers corresponding to the \n",
        "    # indices of the tokens in the vocabulary\n",
        "    for t in range(len(X)):\n",
        "        x[t] = np.zeros((vocab_size, 1))\n",
        "        if (X[t] != None):\n",
        "            x[t][X[t]] = 1\n",
        "        \n",
        "        # Compute the outputs a[t] and y_hat[t] of the RNN Cell\n",
        "        a[t], y_hat[t], _ = rnn_cell_forward(x[t], a[t-1],parameters)\n",
        "        \n",
        "        # This computes the loss for a single time step\n",
        "        # Due to the for loop the loss is accumulated for the whole output sequence\n",
        "        loss -= np.log(y_hat[t][Y[t], 0])\n",
        "    cache = (y_hat, a, x)\n",
        "    return loss, cache\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rnn_cell_backward(dy, gradients, parameters, x, a, a_prev):\n",
        "    # Compute the gradients of a given RNN Cell w.r.t. Wya, by, etc.\n",
        "    # The gradients for an RNN Cell depend on quite a lot: \n",
        "    # - dy: deviation between predictions and true values \n",
        "    # - parameters: the parameters used to compute the predictions\n",
        "    # - x: the vectors of the input sequence\n",
        "    # - a: the activations computed in this cell\n",
        "    # - a_prev: the activations passed from the previous cell \n",
        "\n",
        "    # Note: By using the += operator here, the gradients of all RNN cells \n",
        "    # are accumulated during the for loop in rnn_backward()\n",
        "\n",
        "    gradients['dWya'] += np.dot(dy, a.T)\n",
        "    gradients['dby'] += dy\n",
        "    da = np.dot(parameters['Wya'].T, dy) +         gradients['da_next']  # backprop into h\n",
        "    daraw = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
        "    gradients['db'] += daraw\n",
        "    gradients['dWax'] += np.dot(daraw, x.T)\n",
        "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
        "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def rnn_backward(X, Y, parameters, cache):\n",
        "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
        "    to the parameters. It returns also all the hidden states.\"\"\"\n",
        "\n",
        "    gradients = {}\n",
        "    (y_hat, a, x) = cache\n",
        "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
        "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
        "    gradients['db'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
        "    gradients['da_next'] = np.zeros_like(a[0])\n",
        "\n",
        "    for t in reversed(range(len(X))):     \n",
        "        dy = np.copy(y_hat[t])\n",
        "\n",
        "        # Compute the deviation between prediction and true value \n",
        "        dy[Y[t]] -= 1\n",
        "\n",
        "        # Compute the gradients needed for parameter updating\n",
        "        # Due to the for loop gradients for all RNN cells are accumulated into gradients\n",
        "        gradients = rnn_cell_backward(\n",
        "            dy, gradients, parameters, x[t], a[t], a[t-1])\n",
        "    \n",
        "    return gradients, a\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_parameters(parameters, gradients, lr):\n",
        "    '''\n",
        "    This function adjusts the parameters during backpropagation \n",
        "    such that in the next iteration (hopefully) a lower loss function value is achieved.\n",
        "    A learning rate below 1 (e.g. lr = 0.01) ensures that parameters are not adjusted \n",
        "    too much with respect to the gradients calculated from the samples in the current batch.\n",
        "\n",
        "    Note: The parameters are shared across the whole RNN. Therefore this function \n",
        "    updates the parameters only once for each training example. (See use of update_parameters() in optimize())\n",
        "    '''\n",
        "    parameters['Wax'] += -lr * gradients['dWax']\n",
        "    parameters['Waa'] += -lr * gradients['dWaa']\n",
        "    parameters['Wya'] += -lr * gradients['dWya']\n",
        "    parameters['ba'] += -lr * gradients['db']\n",
        "    parameters['by'] += -lr * gradients['dby']\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  '''\n",
        "  Used as the output layer for each step of the RNN.  \n",
        "  With 27 tokens in the vocabulary the output vector of softmax has 27 elements.\n",
        "  '''\n",
        "  e_x = np.exp(x - np.max(x)) \n",
        "  return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sample(parameters, char_to_ix, seed):\n",
        "    '''\n",
        "    This uses the RNN Cell with computes the vocabulary\n",
        "    '''\n",
        "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]    \n",
        "\n",
        "    # Use a vector of zeros for x<1> and a<0> as an input to the first RNN Cell \n",
        "    # to generate the first letter of the generated sequence\n",
        "    x = np.zeros((vocab_size, 1)) \n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    indices = []\n",
        "    idx = -1 \n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']  \n",
        "    \n",
        "    #Iteratively generate and append letters i.e. token (more precicely the corresponding integers) \n",
        "    # using the RNN. Stop only when the generated token is the EOS token '\\n' or\n",
        "    # if the output sequence becomes longer than 50 letters.\n",
        "\n",
        "    while (idx != newline_character and counter != 50):  \n",
        "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
        "        z = np.dot(Wya, a) + by\n",
        "        y = softmax(z)\n",
        "\n",
        "        # randomly sample an integer (corresponding to a letter) using the probabilities in the output vector y    \n",
        "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel()) \n",
        "\n",
        "        # Append the integer to our output sequence\n",
        "        indices.append(idx)\n",
        "        \n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[idx] = 1\n",
        "        \n",
        "        a_prev = a\n",
        "        \n",
        "        counter +=1\n",
        "        \n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "    \n",
        "    return indices\n",
        "\n",
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    '''\n",
        "    This executes one step of the optimization to train the model. Particularly, \n",
        "    one step here means that one training examples is shown to the model and \n",
        "    parameters are updated w.r.t. this training example. \n",
        "    \n",
        "    Arguments:\n",
        "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
        "    a_prev -- previous hidden state.\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    learning_rate -- learning rate for the model.\n",
        "    \n",
        "    Returns:\n",
        "    loss -- value of the loss function (cross-entropy)\n",
        "    gradients -- python dictionary containing:\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
        "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
        "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
        "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
        "    '''\n",
        "    # Forward pass and calculation of the loss in this iteration\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "    \n",
        "    # Retrieve gradients of the loss function wrt the parameters\n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "    \n",
        "    gradients = clip(gradients, 5)   # The clip function supports faster training. It can in principle be left out.\n",
        "    \n",
        "    # Update the RNN parameters\n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "        \n",
        "    return loss, gradients, a[len(X)-1]\n",
        "\n",
        "\n",
        "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
        "    '''\n",
        "    This function trains an RNN text generation model. \n",
        "    As a side effect the function also prints every 2000 parameter upgrades \n",
        "    a bunch of generated dino name examples to get some intuition about how well \n",
        "    the model already captures the structure of dino names. \n",
        "\n",
        "    Inputs:\n",
        "      data: string of dino names separated by \"\\n\" (=newline symbol) \n",
        "      ix_to_char: dictionary to lookup the token(letter) corresponding to its numerical value\n",
        "      char_to_ix: dictionary to lookup the numerical value correspondiong to its token(letter)\n",
        "      num_iterations: Number of iterations to do backpropagation and parameter updates\n",
        "      n_a: Number of activations in each RNN Cell\n",
        "      dino_names: number of generated dino names after every 2000th iteration\n",
        "      vocab_size: number of tokens in the vocabulary\n",
        "\n",
        "    Output: \n",
        "    This function returns a dictionary containing the parameters of the fully trained model.\n",
        "    '''\n",
        "    n_x, n_y = vocab_size, vocab_size  # Each time step takes a character as input and outputs a character. The number of characters considered is determined by vocab_size.\n",
        "    \n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "    \n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "    \n",
        "    with open(data_path, 'r')  as f:\n",
        "        examples = f.readlines()\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "    \n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "    \n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    # This loop runs through all training examples. \n",
        "    # If num_iterations is set higher than thee number of training examples the loop \n",
        "    # runs mulitple time through the training examples.\n",
        "    for j in range(num_iterations):\n",
        "        index = j % len(examples)\n",
        "        # Encode training examples as vocabulary indices: \n",
        "        # Get vocabulary indices of the tokens for each example input sequence (X)\n",
        "        # and output sequence (Y), where Y is the same as the input sequence X but \n",
        "        # shifted to the right by one and ending with the [EOS]-token \"\\n\"\n",
        "        # Note: The encoding of index numbers as dummy vectors happens inside the \n",
        "        # optimize function, more precisely inside the rnn_forward function that is \n",
        "        # called in the optimize function.\n",
        "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
        "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
        "\n",
        "        # Perform one optimization step \n",
        "        # This includes: Forward-prop -> Backward-prop -> Gradient Clipping -> Update Parameters\n",
        "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
        "        \n",
        "        # The loss smoothing is used to display a smoother loss evolution\n",
        "        loss = smooth(loss, curr_loss)\n",
        "        \n",
        "        # Every 2000th Iteration, generate \"n\" characters thanks to sample() to \n",
        "        # check if the model is learning the structure of dino names properly\n",
        "        if j % 2000 == 0:\n",
        "            \n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "            \n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                \n",
        "                # Use the sample function to generate one output sequence\n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "\n",
        "                # Transform the vocabulary indices in sampled_indices to print text\n",
        "                txt = ''.join(ix_to_char[ix] for ix in sampled_indices)\n",
        "                txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "                print('%s' % (txt, ), end='')\n",
        "                \n",
        "                # change random seed to ensure sampling different output sequences \n",
        "                # each time a dino name is generated\n",
        "                seed += 1  \n",
        "      \n",
        "            print('\\n')\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Some tricks that help faster training, but are not essential components of an RNN\n",
        "################################################################################\n",
        "\n",
        "def smooth(loss, cur_loss):\n",
        "  '''\n",
        "  Loss smoothing is not an essential component for an RNN, but it helps to \n",
        "  accelerate the training. I use loss smoothing mainly to make the training \n",
        "  fast enough to show the full training run conveniently during the lecture.\n",
        "  '''\n",
        "  return loss * 0.999 + cur_loss * 0.001\n",
        "\n",
        "def get_initial_loss(vocab_size, seq_length):\n",
        "  '''\n",
        "  This generates a starting value for the loss. \n",
        "  A starting value is needed to conveniently use loss smoothing. \n",
        "  The formula used ensures a reasonable start values for loss. \n",
        "  As with loss smoothing this is not an essential component for an RNN, \n",
        "  but it helps to accelerate the training. I use loss smoothing mainly to make the training \n",
        "  fast enough to show the full training run conveniently during the lecture.\n",
        "  '''\n",
        "  return -np.log(1.0/vocab_size)*seq_length\n",
        "\n",
        "def clip(gradients, maxValue):\n",
        "    ''' Gradient Clipping:\n",
        "        Gradient clipping is not an essential component for an RNN, but a general way to \n",
        "        support a more stable training process for neural networks in general \n",
        "        (stable training process = the loss function does not wildly jump up and down with each batch).\n",
        "        Gradient clipping is used mainly to make the training fast enough to show \n",
        "        the full training run conveniently during the lecture.\n",
        "\n",
        "        Background: Sometimes extremely high or low gradients occur during a training iteration. \n",
        "        These high gradients would have a lot of \"changing power\" on the parameters \n",
        "        during the parameter updates and the loss function, correspondingly.\n",
        "        In order to avoid too strong parameter updates, the gradients are clipped \n",
        "        i.e. restrained to stay in a certain range, here the range -maxValue, maxValue \n",
        "        Only after gradients are clipped the parameter updates are performed      \n",
        "    '''\n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "   \n",
        "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
        "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
        "    \n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "def print_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "    print('%s' % (txt, ), end='')\n",
        "\n",
        "#########################################\n",
        "\n",
        "# Load training data (lots of dinosaur names eparated by \"\\n\" i.e. our [EOS] token\n",
        "data = open(data_path, 'r').read()\n",
        "\n",
        "# Transform training dino names to lower case (to keep the vocabulary at 27 parameters)\n",
        "data= data.lower() \n",
        "\n",
        "# Get the set of all unique tokens\n",
        "chars = list(set(data))\n",
        "\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
        "\n",
        "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
        "\n",
        "# Now train the model, after each 2000th iteration a bunch of dino names is generated\n",
        "# to get an intuition of how well the RNN already captures the structure of dino names\n",
        "# Finally the parameters of the trained model are stored in trained_parameters\n",
        "trained_parameters = model(data, ix_to_char, char_to_ix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}